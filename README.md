# Zero-Latency Voice RAG Agent

A production-grade Voice AI Agent for CCaaS platforms that achieves **TTFB < 800ms** for synthesized audio responses from large technical manuals.

## ðŸš€ Quick Start

```bash
# Install dependencies
pip install -e .

# Set up environment
cp .env.example .env
# Edit .env with your Gemini API key

# Download and index manuals
python scripts/download_manuals.py
python scripts/build_index.py

# Run the server
python -m src.main
## ðŸ—ï¸ Architecture

The system is split into a modern **Vite/React** frontend and a **FastAPI** backend.

### ðŸš€ Quick Start

#### 1. Start the Backend
```bash
# From the root directory
$env:PYTHONPATH = "."
uv run uvicorn src.simple_server:app --reload
```

#### 2. Start the Frontend
```bash
# In a new terminal
cd frontend-react
npm install
npm run dev
```

The frontend will be available at `http://localhost:5173`.
The backend provides a streaming API at `http://localhost:8000/query/stream`.

---

## ðŸ› ï¸ Key Technologies
- **Frontend**: React 19, Vite 7, Tailwind CSS v4, Framer Motion, Lucide Icons.
- **Backend**: FastAPI, OpenRouter (Trinity LLM), Web Speech API (Browser-side).
- **Features**: SSE Streaming, Speculative Execution (Logic), Contextual Fillers, Voice Optimization.

---

## ðŸ—ï¸ Components Breakdown
- `src/query/rewriter.py`: Handles conversational context and pronoun resolution.
- `src/retrieval/`: Hybrid search (Dense + BM25) with cross-encoder reranking.
- `src/voice/optimizer.py`: Post-processes LLM output for natural speech.
- `src/generation/filler_generator.py`: Generates fillers to mask retrieval latency.
- **Speculative Execution**: Starts retrieval from partial ASR transcripts
- **Hybrid Search**: Dense vectors + BM25 with cross-encoder reranking  
- **Voice Optimization**: Converts technical text to natural speech
- **Filler Generation**: Eliminates perceived silence during processing

## ðŸ“Š Latency Breakdown

| Component | Target | Actual |
|-----------|--------|--------|
| ASR Processing | 300ms | - |
| Query Rewriting | 50ms | - |
| Hybrid Retrieval | 100ms | - |
| Reranking | 150ms | - |
| LLM First Token | 100ms | - |
| TTS First Byte | 50ms | - |
| **Total TTFB** | **<800ms** | - |

## ðŸ› ï¸ Tech Stack

- **ASR**: Whisper (local)
- **LLM**: Google Gemini
- **TTS**: Piper (local)
- **Vector DB**: FAISS
- **Framework**: FastAPI + WebSockets

## ðŸ“ Project Structure

```
src/
â”œâ”€â”€ asr/          # Streaming speech recognition
â”œâ”€â”€ query/        # Query rewriting & context
â”œâ”€â”€ retrieval/    # Hybrid search + reranking
â”œâ”€â”€ generation/   # LLM streaming responses
â”œâ”€â”€ voice/        # TTS optimization
â””â”€â”€ pipeline/     # Orchestration engine
```

## ðŸ“œ License

MIT
